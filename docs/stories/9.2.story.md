# Story 9.2: PostgreSQL Migration & Database Infrastructure

## Story Information
**Epic:** 9 - Critical Security & Infrastructure Hardening  
**Story ID:** 9.2  
**Priority:** Critical (P0)  
**Estimated Effort:** 8 hours  
**Dependencies:** Story 9.1 (Authentication System)  
**Sub-Stories:** 9.2.1, 9.2.2  

## User Story
As a database architect,  
I want to migrate from SQLite to PostgreSQL with proper connection pooling,  
So that the system can handle production trading loads with ACID compliance.

## Problem Statement
SQLite is completely inadequate for production trading systems. It lacks proper concurrency support, has limited transaction isolation, no connection pooling, and cannot handle the concurrent read/write loads required for high-frequency trading. The current single-file database approach creates a single point of failure and offers no scalability path. For a system handling $100k+ in capital, we need enterprise-grade database infrastructure with proper backup, replication, and performance capabilities.

## Acceptance Criteria
1. PostgreSQL 15+ setup with proper configuration tuning
2. Connection pooling with PgBouncer (transaction mode)
3. Read replicas for analytics queries
4. Automated migration from SQLite with zero data loss
5. Partitioned tables for time-series data (orders, trades)
6. Proper indexes for all query patterns
7. Query performance monitoring with pg_stat_statements
8. Automated vacuum and analyze scheduling
9. Point-in-time recovery capability (PITR)
10. Database performance baseline (<5ms for critical queries)

## Technical Implementation Details

### Core Components
```python
# New files to create:
# genesis/database/postgres_manager.py - PostgreSQL connection manager
# genesis/database/migration_engine.py - SQLite to PostgreSQL migration
# genesis/database/query_optimizer.py - Query performance monitoring
# genesis/database/partition_manager.py - Table partitioning management
# genesis/database/backup_manager.py - Database backup utilities
```

### PostgreSQL Configuration
```yaml
# postgresql.conf optimizations
shared_buffers = '1GB'
effective_cache_size = '3GB'
work_mem = '16MB'
maintenance_work_mem = '512MB'
wal_buffers = '16MB'
max_wal_size = '4GB'
min_wal_size = '1GB'
checkpoint_completion_target = 0.9
random_page_cost = 1.1
effective_io_concurrency = 200

# Connection settings
max_connections = 200
shared_preload_libraries = 'pg_stat_statements'

# Logging
log_destination = 'csvlog'
logging_collector = on
log_statement = 'all'
log_duration = on
log_min_duration_statement = 100ms
```

### Files to Modify
- `genesis/database/connection.py` - Replace SQLite with PostgreSQL
- `genesis/models/base.py` - Update ORM for PostgreSQL-specific features
- `genesis/api/trading.py` - Update for connection pooling
- `genesis/config/database.py` - PostgreSQL configuration
- `requirements.txt` - Add asyncpg, psycopg2-binary dependencies
- `docker-compose.yml` - Add PostgreSQL service

### Database Schema Migration
```sql
-- Create optimized schema for PostgreSQL
CREATE DATABASE genesis_trading
    WITH OWNER = genesis
    ENCODING = 'UTF8'
    LC_COLLATE = 'en_US.UTF-8'
    LC_CTYPE = 'en_US.UTF-8'
    TEMPLATE = template0;

-- Create partitioned tables for time-series data
CREATE TABLE orders (
    id BIGSERIAL,
    created_at TIMESTAMPTZ NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    side VARCHAR(4) NOT NULL CHECK (side IN ('buy', 'sell')),
    type VARCHAR(10) NOT NULL CHECK (type IN ('market', 'limit', 'stop')),
    quantity DECIMAL(20, 8) NOT NULL CHECK (quantity > 0),
    price DECIMAL(20, 8),
    status VARCHAR(20) NOT NULL,
    exchange_order_id VARCHAR(100),
    metadata JSONB,
    PRIMARY KEY (id, created_at)
) PARTITION BY RANGE (created_at);

-- Create monthly partitions (automated via partition_manager.py)
CREATE TABLE orders_2024_01 PARTITION OF orders 
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- Performance indexes
CREATE INDEX CONCURRENTLY idx_orders_symbol_created ON orders (symbol, created_at DESC);
CREATE INDEX CONCURRENTLY idx_orders_status ON orders (status) WHERE status = 'pending';
CREATE INDEX CONCURRENTLY idx_orders_exchange_id ON orders (exchange_order_id) WHERE exchange_order_id IS NOT NULL;

-- Enable pg_stat_statements for query monitoring
CREATE EXTENSION IF NOT EXISTS pg_stat_statements;
```

## Implementation Checklist

### Phase 1: PostgreSQL Setup & Configuration (2 hours)
- [ ] Install PostgreSQL 15+ with proper configuration
- [ ] Setup connection pooling with PgBouncer
- [ ] Configure read replica for analytics workload
- [ ] Optimize PostgreSQL settings for trading workload
- [ ] Setup monitoring with pg_stat_statements
- [ ] Create database users with proper permissions

### Phase 2: Schema Migration (3 hours)
- [ ] Design optimized PostgreSQL schema
- [ ] Create partitioned tables for time-series data
- [ ] Add all necessary indexes for query patterns
- [ ] Implement data type conversions from SQLite
- [ ] Create migration scripts with rollback capability
- [ ] Validate schema performance with sample data

### Phase 3: Data Migration (2 hours)
- [ ] Build SQLite to PostgreSQL migration engine
- [ ] Implement zero-downtime migration strategy
- [ ] Add data integrity verification checks
- [ ] Create migration progress monitoring
- [ ] Test migration with production-sized datasets
- [ ] Implement automated rollback on migration failure

### Phase 4: Performance Optimization (1 hour)
- [ ] Implement automated partition management
- [ ] Setup query performance monitoring
- [ ] Configure automatic vacuum and analyze
- [ ] Add connection pool monitoring
- [ ] Implement query caching strategies
- [ ] Load test with production query patterns

## Testing Requirements

### Unit Tests
- [ ] PostgreSQL connection manager functionality
- [ ] Migration engine with various data types
- [ ] Partition management automation
- [ ] Connection pooling behavior
- [ ] Query optimization utilities
- [ ] Backup and restore procedures

### Integration Tests
- [ ] Complete SQLite to PostgreSQL migration
- [ ] Connection pooling under concurrent load
- [ ] Read replica consistency verification
- [ ] Partition boundary testing
- [ ] Transaction isolation level testing
- [ ] Point-in-time recovery testing

### Performance Tests
- [ ] <5ms query performance for critical operations
- [ ] 1000+ concurrent connection handling
- [ ] Partition pruning effectiveness
- [ ] Connection pool efficiency
- [ ] Read replica lag measurement
- [ ] Bulk insert performance testing

### Data Integrity Tests
- [ ] Migration data accuracy verification
- [ ] Transaction ACID property testing
- [ ] Constraint enforcement validation
- [ ] Foreign key relationship integrity
- [ ] JSON data type handling
- [ ] Decimal precision maintenance

## Dependencies
- **Requires:** Story 9.1 (for updated authentication schema)
- **Blocks:** Story 9.3 (Vault needs database credentials)
- **External:** PostgreSQL 15+, PgBouncer
- **Internal:** Migration downtime coordination

## Definition of Done
- [ ] PostgreSQL 15+ running with optimized configuration
- [ ] All SQLite data successfully migrated with zero loss
- [ ] Connection pooling operational with PgBouncer
- [ ] Partitioned tables handling time-series data efficiently
- [ ] All critical queries performing <5ms at p99
- [ ] Read replica configured and synchronized
- [ ] Query performance monitoring active
- [ ] Automated maintenance tasks scheduled
- [ ] Point-in-time recovery capability verified
- [ ] Load testing confirms 1000+ TPS capability
- [ ] Migration rollback procedures documented
- [ ] Database monitoring and alerting configured

## Risk Mitigation
- **Migration Downtime:** Implement hot migration with minimal service interruption
- **Data Loss Risk:** Triple verification of migration accuracy with checksums
- **Performance Regression:** Baseline current performance before migration
- **Connection Pool Issues:** Gradual rollout with monitoring
- **Disk Space:** Ensure sufficient storage for PostgreSQL and migration

## Success Metrics
- Zero data loss during migration (100% accuracy verification)
- <5ms response time for 99% of critical queries
- Support for 1000+ transactions per second
- <1% connection pool utilization during normal operations
- 99.9% database uptime post-migration
- <100ms read replica lag at 95th percentile

## PostgreSQL Specific Optimizations

### Trading-Optimized Settings
```sql
-- Enable JIT for complex queries (disabled for consistent latency)
SET jit = off;

-- Optimize for trading workload
SET statement_timeout = '10s';
SET lock_timeout = '5s';
SET idle_in_transaction_session_timeout = '30s';

-- Connection-level optimizations
SET tcp_keepalives_idle = 600;
SET tcp_keepalives_interval = 30;
SET tcp_keepalives_count = 3;
```

### Monitoring Queries
```sql
-- Query performance monitoring
SELECT query, calls, total_time, mean_time, rows
FROM pg_stat_statements
WHERE query NOT LIKE '%pg_stat_statements%'
ORDER BY total_time DESC LIMIT 10;

-- Connection pool status
SELECT database, user, active, waiting, max_connections
FROM pg_pool_processes;

-- Partition status
SELECT schemaname, tablename, attname, n_distinct, correlation
FROM pg_stats
WHERE tablename LIKE 'orders_%'
ORDER BY schemaname, tablename;
```

## Notes
This is a **CRITICAL INFRASTRUCTURE** story that fundamentally transforms the system's data storage capability. SQLite is completely unsuitable for production trading systems due to concurrency limitations and lack of enterprise features.

The migration must be executed with extreme care as any data loss would be catastrophic. The implementation includes comprehensive verification steps and rollback procedures.

PostgreSQL provides:
- MVCC for proper concurrency
- Advanced indexing (GiST, GIN, BRIN)
- Partitioning for time-series data
- Connection pooling support
- Point-in-time recovery
- Read replicas for scaling
- Advanced query optimization
- Enterprise-grade reliability

This migration enables all subsequent scaling and reliability improvements in the system.