# Story 10.6.1: Backtesting Engine Implementation

## Status
Approved

## Story
**As a** quant researcher,
**I want** a comprehensive backtesting engine to validate strategies before live deployment,
**So that** we can ensure positive expectancy and proper risk-adjusted returns.

**Epic:** Epic 10 - Core Trading Brain Implementation
**Type:** Parallel-Safe
**Branch:** feature/10-6-1
**Estimated Hours:** 10
**Developer Assignment:** Developer 3

## Acceptance Criteria
1. BacktestEngine class with historical replay
2. Realistic slippage and fee modeling
3. Order fill simulation with market impact
4. Portfolio tracking through time
5. Drawdown analysis and limits
6. Monte Carlo simulation support
7. Walk-forward optimization
8. Out-of-sample validation
9. Multi-strategy comparison
10. Automated report generation

## Tasks / Subtasks

- [ ] Create BacktestEngine core class (AC: 1)
  - [ ] Implement historical data replay mechanism
  - [ ] Add time-based event simulation
  - [ ] Create strategy execution loop
- [ ] Implement realistic market simulation (AC: 2, 3)
  - [ ] Create slippage models (linear, square-root)
  - [ ] Add fee calculation (maker/taker, tiered)
  - [ ] Implement market impact modeling
- [ ] Build order fill simulator (AC: 3)
  - [ ] Create realistic fill prices
  - [ ] Add partial fill support
  - [ ] Implement order rejection scenarios
- [ ] Create Portfolio tracking system (AC: 4)
  - [ ] Track positions through time
  - [ ] Calculate P&L continuously
  - [ ] Implement mark-to-market valuation
- [ ] Add drawdown analysis (AC: 5)
  - [ ] Calculate maximum drawdown
  - [ ] Track drawdown duration
  - [ ] Implement drawdown-based stops
- [ ] Implement Monte Carlo simulation (AC: 6)
  - [ ] Create parameter randomization
  - [ ] Generate confidence intervals
  - [ ] Build statistical analysis
- [ ] Add walk-forward optimization (AC: 7)
  - [ ] Implement rolling window analysis
  - [ ] Create parameter optimization
  - [ ] Add overfitting detection
- [ ] Build out-of-sample validation (AC: 8)
  - [ ] Split data for training/testing
  - [ ] Implement cross-validation
  - [ ] Add performance degradation checks
- [ ] Create strategy comparison framework (AC: 9)
  - [ ] Build side-by-side comparison
  - [ ] Add correlation analysis
  - [ ] Implement ranking system
- [ ] Build report generation system (AC: 10)
  - [ ] Create HTML/PDF reports
  - [ ] Add performance charts
  - [ ] Generate statistical summaries

## Dev Notes

### Testing Standards

**Test File Locations:**
- Unit tests: `tests/unit/test_backtest_engine.py`
- Strategy tests: `tests/backtesting/test_strategies.py`
- Integration tests: `tests/integration/test_backtest_flow.py`

**Testing Framework:**
- Framework: pytest 8.0.0
- Coverage requirement: >85% for backtesting module
- Mock data: Use fixtures for historical data
- Validation: Compare against known results

### Relevant Source Tree

```plaintext
genesis/
├── backtesting/               # NEW MODULE
│   ├── __init__.py
│   ├── engine.py             # Core backtesting engine
│   ├── data_provider.py      # Historical data loading
│   ├── execution_simulator.py # Order fill simulation
│   ├── portfolio.py          # Portfolio tracking
│   ├── slippage_models.py    # Slippage calculations
│   ├── monte_carlo.py        # Monte Carlo simulation
│   ├── walk_forward.py       # Walk-forward optimization
│   └── report_generator.py   # Report generation
├── strategies/
│   └── base.py               # BaseStrategy interface
├── core/
│   └── models.py             # Signal, Order, Position models
└── analytics/
    └── performance_metrics.py # Interface for metrics (10.6.2)
```

### Architecture Context

**Technology Stack:**
- Python 3.11.8 with asyncio
- pandas 2.2.0 for time series manipulation
- numpy 1.26.3 for numerical calculations
- SQLite for storing backtest results
- matplotlib/plotly for report visualization

**Performance Requirements:**
- Process 1000 trades in <1 second
- Handle 5 years of minute data
- Support 100+ concurrent backtests
- Memory usage <2GB per backtest
- Report generation <5 seconds

**Data Structures:**
```python
@dataclass
class BacktestResult:
    strategy: str
    start_date: datetime
    end_date: datetime
    initial_capital: Decimal
    final_capital: Decimal
    total_trades: int
    winning_trades: int
    losing_trades: int
    max_drawdown: Decimal
    sharpe_ratio: float
    sortino_ratio: float
    calmar_ratio: float
    profit_factor: float
    win_rate: float
    avg_win: Decimal
    avg_loss: Decimal
    portfolio_history: List[PortfolioSnapshot]
    trades: List[Trade]
```

**Integration Points:**
- `genesis/strategies/base.py` - Strategy interface
- `genesis/core/models.py` - Domain models
- `genesis/analytics/performance_metrics.py` - Metrics calculation (10.6.2)
- `genesis/data/` - Historical data access

**Slippage Models:**
```python
class LinearSlippageModel:
    """Linear slippage based on order size."""
    def calculate(self, price: Decimal, quantity: Decimal) -> Decimal:
        return price * (1 + self.slippage_pct * quantity / avg_volume)

class SquareRootSlippageModel:
    """Square-root market impact model."""
    def calculate(self, price: Decimal, quantity: Decimal) -> Decimal:
        return price * (1 + self.impact_coeff * sqrt(quantity / adv))
```

**Monte Carlo Parameters:**
- Number of simulations: 1000 default
- Parameter ranges: ±20% from base
- Random seeds for reproducibility
- Confidence intervals: 95% and 99%

### Code Patterns to Follow

**Async Pattern for Backtesting:**
```python
async def run_backtest(
    self,
    strategy: BaseStrategy,
    start_date: datetime,
    end_date: datetime,
    initial_capital: Decimal = Decimal("1000")
) -> BacktestResult:
    """Run historical backtest for a strategy."""
    # Load historical data
    data = await self.data_provider.load_data(
        symbols=strategy.symbols,
        start_date=start_date,
        end_date=end_date,
        resolution='1m'
    )
    
    # Initialize portfolio
    portfolio = Portfolio(initial_capital)
    
    # Replay market data
    async for timestamp, market_snapshot in data:
        # Generate signals
        signal = await strategy.analyze(market_snapshot)
        
        if signal:
            # Simulate execution
            fill = await self.execution_simulator.simulate_fill(
                signal=signal,
                market_data=market_snapshot,
                slippage_model=self.slippage_model
            )
            
            # Update portfolio
            portfolio.process_fill(fill)
        
        # Mark to market
        portfolio.mark_to_market(market_snapshot)
        
        # Check drawdown limits
        if portfolio.drawdown > strategy.max_drawdown:
            logger.warning(
                "max_drawdown_exceeded",
                strategy=strategy.name,
                drawdown=portfolio.drawdown
            )
            break
    
    # Calculate final metrics
    return self._generate_results(strategy, portfolio)
```

**Portfolio Tracking:**
```python
class Portfolio:
    def __init__(self, initial_capital: Decimal):
        self.cash = initial_capital
        self.positions = {}
        self.history = []
        self.trades = []
        
    def process_fill(self, fill: Fill) -> None:
        """Process a trade fill."""
        # Update cash
        self.cash -= fill.value + fill.fee
        
        # Update position
        if fill.symbol in self.positions:
            self.positions[fill.symbol].add_fill(fill)
        else:
            self.positions[fill.symbol] = Position(fill)
        
        # Record trade
        self.trades.append(fill)
        
        # Take snapshot
        self.history.append(self.snapshot())
```

### Files to Modify/Create
```
├── genesis/
│   ├── backtesting/
│   │   ├── __init__.py [CREATE]
│   │   ├── engine.py [CREATE]
│   │   ├── data_provider.py [CREATE]
│   │   ├── execution_simulator.py [CREATE]
│   │   └── portfolio.py [CREATE]
└── tests/
    ├── unit/
    │   └── test_backtest_engine.py [CREATE]
    └── backtesting/
        └── test_strategies.py [CREATE]
```

### Code Modules Owned
- **Primary:** `genesis/backtesting/engine.py`
- **Secondary:** `genesis/backtesting/execution_simulator.py`
- **Tertiary:** `genesis/backtesting/data_provider.py`
- **No Touch:** performance_metrics.py (Dev 4), strategies

### Dependencies Context

**Must Complete Before Starting
- None (can work with interface definitions)

### Can Start But Needs Later
- Story 10.2 (Strategies to test)
- Story 10.6.2 (Performance metrics)

### Parallel Stories (Same Time)
- Story 10.6.2 (Performance Metrics - Developer 4)
- Story 10.4.1 (VWAP - Different team)
- Story 10.1.3 (Spread Tracker - Different team)

## Validation & Testing

### Testing Requirements

**Unit Tests Required:**
- `test_backtest_engine.py` - Core engine functionality
- `test_execution_simulator.py` - Fill simulation accuracy
- `test_portfolio.py` - Portfolio calculations
- `test_slippage_models.py` - Slippage model validation
- `test_monte_carlo.py` - Monte Carlo simulation

**Validation Tests:**
- Known strategy results comparison
- Slippage model accuracy against real data
- Fee calculation correctness
- Portfolio P&L accuracy
- Drawdown calculation validation

**Performance Tests:**
- 1000 trades processed in <1 second
- 5 years of minute data handling
- Memory usage <2GB per backtest
- Report generation <5 seconds

### Validation Steps

1. Test with known profitable strategy
2. Validate slippage calculations
3. Verify fee calculations
4. Check portfolio tracking accuracy
5. Validate drawdown calculations
6. Test Monte Carlo convergence
7. Verify walk-forward optimization
8. Check out-of-sample performance
9. Compare multiple strategies
10. Generate and review reports

## Implementation Checklist
- [ ] Create feature branch via worktree
- [ ] Create BacktestEngine class
- [ ] Implement HistoricalDataProvider
- [ ] Create ExecutionSimulator
- [ ] Add slippage models
- [ ] Implement Portfolio tracker
- [ ] Add drawdown monitoring
- [ ] Create Monte Carlo simulator
- [ ] Implement walk-forward logic
- [ ] Add report generator
- [ ] Write unit tests
- [ ] Test with sample strategies
- [ ] Validate simulation accuracy
- [ ] Create pull request
- [ ] Peer review from Story 10.6.2 developer
- [ ] Merge to main branch

## Worktree Setup Commands
```bash
# Developer 3 executes:
cd /path/to/main/repo
git worktree add -b feature/10-6-1 ../worktree-10-6-1
cd ../worktree-10-6-1

# After completion:
git push origin feature/10-6-1
# Create PR for review
```

## Definition of Done
- [ ] All acceptance criteria met
- [ ] Code review approved
- [ ] All tests passing (>85% coverage)
- [ ] Simulation validated against known results
- [ ] Performance acceptable (<1s per 1000 trades)
- [ ] Documentation complete
- [ ] Branch merged to main

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-09-03 | 1.0 | Initial story creation | Sarah (PO) |
| 2025-09-03 | 1.1 | Enhanced with full Dev Notes and validation | Sarah (PO) |

## Dev Agent Record

### Agent Model Used
_TBD_

### Debug Log References
_TBD_

### Completion Notes List
_TBD_

### File List
_TBD_

## QA Results
_TBD_